!pip install kagglehub tensorflow matplotlib scikit-learn seaborn --quiet

import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models, callbacks

import kagglehub
path = kagglehub.dataset_download("alxmamaev/flowers-recognition")

possible = [
    os.path.join(path, "flowers"),
    os.path.join(path, "flowers-recognition", "flowers"),
    os.path.join(path, "flowers-recognition"),
    path
]

train_dir = None
for p in possible:
    if os.path.isdir(p):
        train_dir = p
        break

if train_dir is None:
    raise FileNotFoundError("Dataset folder not found.")

IMG_SIZE = (150, 150)
BATCH_SIZE = 32
SEED = 42

datagen = ImageDataGenerator(
    rescale=1.0/255.0,
    rotation_range=25,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.15,
    horizontal_flip=True,
    validation_split=0.2
)

train_data = datagen.flow_from_directory(
    train_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training',
    shuffle=True,
    seed=SEED
)

val_data = datagen.flow_from_directory(
    train_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation',
    shuffle=False,
    seed=SEED
)

class_indices = train_data.class_indices
class_names = [name for name, idx in sorted(class_indices.items(), key=lambda x: x[1])]
NUM_CLASSES = len(class_names)

model = models.Sequential([
    layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),
    layers.Conv2D(32, (3,3), activation='relu', padding='same'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu', padding='same'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(128, (3,3), activation='relu', padding='same'),
    layers.MaxPooling2D(2,2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(NUM_CLASSES, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
checkpoint = callbacks.ModelCheckpoint("best_flowers_model.h5", save_best_only=True, monitor='val_loss')

history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=15,
    callbacks=[early_stop, checkpoint],
    verbose=1
)

val_steps = int(np.ceil(val_data.samples / BATCH_SIZE))
val_data.reset()
pred_probs = model.predict(val_data, steps=val_steps, verbose=1)
preds = np.argmax(pred_probs, axis=1)
preds = preds[:len(val_data.classes)]
true_labels = val_data.classes

cm = confusion_matrix(true_labels, preds)
print(classification_report(true_labels, preds, target_names=class_names, digits=4))

plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Greens",
            xticklabels=class_names,
            yticklabels=class_names)
plt.title('Confusion Matrix â€” Flower Classification')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

plt.figure(figsize=(6,4))
plt.plot(history.history.get('accuracy', []), label='Train Acc')
plt.plot(history.history.get('val_accuracy', []), label='Val Acc')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training vs Validation Accuracy')
plt.legend()
plt.tight_layout()
plt.show()

plt.figure(figsize=(6,4))
plt.plot(history.history.get('loss', []), label='Train Loss')
plt.plot(history.history.get('val_loss', []), label='Val Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training vs Validation Loss')
plt.legend()
plt.tight_layout()
plt.show()

model.save("flowers_cnn_final.h5")
print("Model Saved")
